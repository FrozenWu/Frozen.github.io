<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">


<link href="https://fonts.loli.net/css?family=Noto+Serif+SC:400,500,700&display=swap&subset=cyrillic,vietnamese" rel="stylesheet">



<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,监督学习,回归,">










<meta name="keywords" content="机器学习,监督学习,回归">
<meta property="og:type" content="article">
<meta property="og:title" content="梯度下降法">
<meta property="og:url" content="http://frozenwu.github.io/2019/09/27/梯度下降法/index.html">
<meta property="og:site_name" content="Frozen&#39;s">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://frozenwu.github.io/2019/09/27/梯度下降法/思维导图.svg">
<meta property="og:image" content="http://frozenwu.github.io/2019/09/27/梯度下降法/fig0.svg">
<meta property="og:image" content="http://frozenwu.github.io/2019/09/27/梯度下降法/theta.png">
<meta property="og:image" content="http://frozenwu.github.io/2019/09/27/梯度下降法/moun.png">
<meta property="og:image" content="http://frozenwu.github.io/2019/09/27/梯度下降法/多项式.png">
<meta property="og:updated_time" content="2020-04-25T05:54:51.559Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="梯度下降法">
<meta name="twitter:image" content="http://frozenwu.github.io/2019/09/27/梯度下降法/思维导图.svg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://frozenwu.github.io/2019/09/27/梯度下降法/">





  <title>梯度下降法 | Frozen's</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

  <a href="https://github.com/FrozenWu" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Frozen's</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://frozenwu.github.io/2019/09/27/梯度下降法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Fangchen Wu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Frozen's">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">梯度下降法</h1>
        

        <div class="post-meta">

            

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-27T18:02:37+08:00">
                2019-09-27
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2020-04-25T13:54:51+08:00">
                2020-04-25
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="//frozenwu.github.io/2019/09/27/梯度下降法/思维导图.svg" alt></p>
<a id="more"></a>
<h1 id="方向导数与梯度"><a href="#方向导数与梯度" class="headerlink" title="方向导数与梯度"></a>方向导数与梯度</h1><h2 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h2><p>我们都知道， 可以利用函数的导数来描绘函数在某一点处的变化率。比如$f(x) = x^2$的导数${f}’(x) = 2x$，当$x_1 = 2$时，${f}’(x_1) = 4$，也就是说函数$f(x)$在$x_1$处的瞬时变化率为$4$。</p>
<h2 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h2><p>方向导数是指在函数定义域的内点，对某一方向求导得到的导数。一元函数似乎没法朝着除了$x$轴以外的方向求导数，但是对于多元函数来说，这是可行的。</p>
<p>设$z = f(x, y); \quad  {(x,y)\in D}; \quad {M_0(x_0,y_0) \in D}$。过点$M_0$做射线$l$，射线 $l$ 上有一点$M’_0(x_0+\delta x,y_0+\delta y)$。</p>
<p>则可以得出：</p>
<script type="math/tex; mode=display">
\begin{align}
\Delta z =&~f(x_0+\delta x,y_0+\delta y)-f(x_0,y_0)\\
\rho =&~|M_0M_0'| = {\sqrt{(\delta x)^2 + (\delta y)^2}}
\end{align}</script><p>看起来需要走过长度$\rho$来实现函数的变化$\Delta z$。</p>
<p><img src="//frozenwu.github.io/2019/09/27/梯度下降法/fig0.svg" style="zoom: 67%;"></p>
<p>当$\delta x,\delta y \rightarrow 0$时，可以将${lim<em>{\rho \to 0}}{\frac{\Delta z}{\rho}}$看作$z = f(x, y)$在$M_0$处沿方向$l$的瞬时变化率，也称函数$z$在点$M_0$处沿方向$l$的方向导数，用$\frac{\partial z}{\partial l}|</em>{M_0}$来表示。</p>
<p>在直角坐标系中，方向导数由下面定理给出计算公式：</p>
<p>若函数$u = f (x, y, z)$ 在点$M_0$处可微， $cos \alpha, cos\beta,cos\gamma$为直线$l$的方向余弦（即$\alpha, \beta, \gamma$分别为为$l$与$x, y, z$轴的夹角），则函数$u$在点$M_0$处沿方向$l$的方向导数必存在，且满足</p>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial l}|_{M_0} = \frac{\partial u}{\partial x}|_{M_0}\times cos\alpha
+\frac{\partial u}{\partial y}|_{M_0}\times cos\beta
+\frac{\partial u}{\partial z}|_{M_0}\times cos\gamma</script><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>将上面的公式用向量的形式来表示：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial z}{\partial l}|_{M_0} =& 
\frac{\partial u}{\partial x}|_{M_0}\times cos\alpha
+\frac{\partial u}{\partial y}|_{M_0}\times cos\beta
+\frac{\partial u}{\partial z}|_{M_0}\times cos\gamma\\
=&
\begin{pmatrix}
\frac{\partial u}{\partial x} &\frac{\partial u}{\partial y}  &\frac{\partial u}{\partial z} 
\end{pmatrix}|_{M_0}\cdot
\begin{pmatrix}
cos\alpha & cos\beta & cos \gamma
\end{pmatrix}\\
=&
\begin{vmatrix}
\frac{\partial u}{\partial x} &\frac{\partial u}{\partial y}  &\frac{\partial u}{\partial z} 
\end{vmatrix}_{M_0}\times
\begin{vmatrix}
cos\alpha & cos\beta & cos \gamma
\end{vmatrix}\times cos\theta\\
=& \sqrt{(\frac{\partial u}{\partial x})^2+(\frac{\partial u}{\partial y})^2+(\frac{\partial u}{\partial z})^2} \times 1 \times cos\theta\\
\end{align}</script><p>其中$\begin{pmatrix}<br>cos\alpha &amp; cos\beta &amp; cos \gamma<br>\end{pmatrix}$为单位向量，模长为$1$，$\theta$为向量$\begin{pmatrix}<br>\frac{\partial u}{\partial x} &amp;\frac{\partial u}{\partial y}  &amp;\frac{\partial u}{\partial z}<br>\end{pmatrix}|_{M_0}$与射线$l$的夹角</p>
<p>令：$\sqrt{(\frac{\partial u}{\partial x})^2+(\frac{\partial u}{\partial y})^2+(\frac{\partial u}{\partial z})^2} = t$ ，则：</p>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial l}|_{M_0} = t \times cos\theta</script><p>由上式可知：当$\theta$越小时，方向导数越大，我们将$\begin{pmatrix}<br>\frac{\partial u}{\partial x} &amp;\frac{\partial u}{\partial y}  &amp;\frac{\partial u}{\partial z}<br>\end{pmatrix}|<em>{M_0}$称为<strong>函数$u$在$M_0$点处的梯度</strong>，记作$\nabla u(x,y,z)|</em>{M_0}$。则$l$方向与梯度方向越接近，$M_0$点处的方向导数越大。</p>
<blockquote>
<p>关于单位向量$\begin{pmatrix}<br>cos\alpha &amp; cos\beta &amp; cos \gamma<br>\end{pmatrix}$：</p>
<p>设$\vec {e}$为三维向量$\vec a=\begin{pmatrix}x&amp;y&amp;z\end{pmatrix}$的单位向量，则$\vec e=\begin{pmatrix}\frac{x}{|a|}&amp;\frac{y}{|a|}&amp;\frac{z}{|a|}\end{pmatrix}=\begin{pmatrix}<br>cos\alpha &amp; cos\beta &amp; cos \gamma<br>\end{pmatrix}$</p>
</blockquote>
<h1 id="梯度下降法-Gradient-Descent"><a href="#梯度下降法-Gradient-Descent" class="headerlink" title="梯度下降法(Gradient Descent)"></a>梯度下降法(Gradient Descent)</h1><h2 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h2><p>让我们通过一个预测住房价格例子来开始：已知的住房面积和住房价格的数据集如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$Size~in~Feet^2(x)$</th>
<th style="text-align:center">$Price~in~1000’s(y)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2104</td>
<td style="text-align:center">460</td>
</tr>
<tr>
<td style="text-align:center">1416</td>
<td style="text-align:center">232</td>
</tr>
<tr>
<td style="text-align:center">1534</td>
<td style="text-align:center">315</td>
</tr>
<tr>
<td style="text-align:center">852</td>
<td style="text-align:center">178</td>
</tr>
<tr>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
</tr>
</tbody>
</table>
</div>
<p>很显然，在这个例子中假设只有房屋的大小才能影响房屋的价格，因为只含有一个特征/输入变量，所以这种问题我们叫做<strong>单变量线性回归</strong>。如果我们要根据以上的数据集预测某一房屋的住房价格，可以不用像初高中那样用线性回归的方式来预测，而用更高级的梯度下降法来预测：</p>
<h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数(Cost Function)"></a>代价函数(Cost Function)</h3><p>我们设$h(x) = \theta_0 + \theta_1 x$为这个问题的假设(Hypothesis)，如何选择参数$\theta_0 ,\theta_1$成为我们现在需要解决的问题。所选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>(modeling error)。</p>
<p>设代价函数$J(\theta_0,\theta_1)$：</p>
<script type="math/tex; mode=display">
J(\theta_0,\theta_1) = \frac{1}{2m} \sum_{i = 1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>我们的目标便是选择出可以使得代价函数（建模误差的平方和）能够最小的模型参数。我们将训练集带入代价函数中，绘制一个等高线图，三个坐标分别为$\theta_0$和$\theta_1$和$J(\theta_0,\theta_1)$：</p>
<p><img src="//frozenwu.github.io/2019/09/27/梯度下降法/theta.png" alt></p>
<p>如果我们能找到代价函数的最小值，我们就能得出$h(x)$的最优假设，那么这个求最小值的方法，我们叫他<strong>梯度下降法</strong>。</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降法又叫<strong>最速下降</strong>法。梯度下降背后的思想是：开始时我们随机选择一个参数的组合$(\theta_0,\theta_1,…\theta_n)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），<strong>选择不同的初始参数组合，可能会找到不同的局部最小值</strong>。</p>
<p><img src="//frozenwu.github.io/2019/09/27/梯度下降法/moun.png" alt></p>
<p>单变量批量梯度下降(batch gradient descent)的算法公式为：</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j -\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)\quad(for~j = 0~and~j = 1)</script><p>其中$\alpha$是学习率(learning rate)，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大；而$\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)$决定了我们可以沿着梯度方向，以最快的效率下降。</p>
<blockquote>
<p>我们在梯度下降时要注意，$\theta_0$与$\theta_1$应该同时迭代，否则下降的方向就不会沿着正确的梯度的方向下降。</p>
</blockquote>
<p>将上文中的假设$ h(x)=\theta_0 + \theta_1 x$带入到梯度下降的公式中求导得：</p>
<script type="math/tex; mode=display">
\begin{align}
j = &~0：~~\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})\\
j = &~1：~~\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)}) - y^{(i)})\cdot x^{(i)})
\end{align}</script><blockquote>
<p>批量梯度下降中<strong>“批量”</strong>是指在每一步梯度下降的迭代中，我们用到了所有样本（求和运算中需要带入所有样本）。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一批训练样本。</p>
<p>当然，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。</p>
</blockquote>
<p>当偏导数大于$0$时，则函数趋于上升趋势，也就是说$\theta_j$需要小一点$J(\theta_0,\theta_1)$才会更小；相反地，当偏导数小于$0$时，则函数趋于下降趋势，也就是说$\theta_j$需要大一点$J(\theta_0,\theta_1)$才会更小。当偏导数越大时，$\alpha\frac{\partial J}{\partial \theta_j}$越大，下降越快，反之函数趋于平稳，下降速度变慢，最后收敛于局部最小值。</p>
<p>值得我们注意的一点是：如果$\alpha$（学习率）过小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，会很浪费我们梯度下降的时间与性能。但如果$\alpha$（学习率）太大，我们就会迈出很大的一步，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，又越过了最低点。直到你发现实际上离最低点越来越远。</p>
<h2 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h2><p>现在，将单变量特征的房价模型增加更多的特征，比如房间卧室数量，房间楼层等，构成一个含有多个变量的多维模型模型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Size$(x_1)$</th>
<th style="text-align:center">Number of bedrooms$(x_2)$</th>
<th style="text-align:center">Number of floors$(x_3)$​</th>
<th style="text-align:center">Age of home$(x_4)$</th>
<th style="text-align:center">Price$(y)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2104</td>
<td style="text-align:center">5</td>
<td style="text-align:center">1</td>
<td style="text-align:center">45</td>
<td style="text-align:center">460</td>
</tr>
<tr>
<td style="text-align:center">1416</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2</td>
<td style="text-align:center">40</td>
<td style="text-align:center">232</td>
</tr>
<tr>
<td style="text-align:center">1534</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2</td>
<td style="text-align:center">30</td>
<td style="text-align:center">315</td>
</tr>
<tr>
<td style="text-align:center">852</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">36</td>
<td style="text-align:center">178</td>
</tr>
<tr>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
<td style="text-align:center">……</td>
</tr>
</tbody>
</table>
</div>
<p>$n~~$代表特征的数量， 在本例中，$n=4$</p>
<p>$x^{(i)}~~$代表第$i$个实例，是特征矩阵中的第$i$行，是一个向量，在本例中：</p>
<script type="math/tex; mode=display">
x^{(2)} = 
\begin{pmatrix}
1416\\3\\2\\40
\end{pmatrix}</script><p>$x_j^{(i)}~~$代表第$i$个实例的第$j$个特征，在本例中$x_3^{(2)} = 2$</p>
<p>支持多变量的先行假设$h$为:</p>
<script type="math/tex; mode=display">
h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n</script><p>为了方便计算，引入$x<em>0 =1$，使得$h</em>\theta(x)$转换为：</p>
<script type="math/tex; mode=display">
h_\theta(x) = \theta_0x_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n</script><p>令$\theta = \begin{pmatrix}\theta<em>0\\theta_1\\theta_2\\dots\\theta_n\end{pmatrix}$，$X = \begin{pmatrix}x_0\x_1\x_2\\dots\x_n\end{pmatrix}$，则$h</em>\theta(x) = \theta^TX$。</p>
<h3 id="多变量梯度下降"><a href="#多变量梯度下降" class="headerlink" title="多变量梯度下降"></a>多变量梯度下降</h3><p>与单变量线性回归相似，多变量线性回归也有一个代价函数$J(\theta)$:</p>
<script type="math/tex; mode=display">
J(\theta_0,\theta_1\dots\theta_n)=\frac{1}{2m} \sum_{i = 1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>多变量线性回归的批量梯度下降算法为：</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j -\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1\dots\theta_n)\quad(for~j = 0,1\dots n)</script><p>代入到梯度下降的公式中求导得：</p>
<script type="math/tex; mode=display">
\begin{align}
j = &~0：~~\frac{\partial}{\partial\theta_0}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)}) - y^{(i)})\cdot x_0^{(i)}) \\
j = &~1：~~\frac{\partial}{\partial\theta_1}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)}) - y^{(i)})\cdot x_1^{(i)})\\
\dots\\
j = &~n：~~\frac{\partial}{\partial\theta_n}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)}) - y^{(i)})\cdot x_n^{(i)})\\
\end{align}</script><h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><p>线性回归不总是可行的，我们可能需要先观察数据，然后再决定准备尝试怎样的模型，有时需要例如二次、三次模型之类的曲线来适应数据：</p>
<p><img src="//frozenwu.github.io/2019/09/27/梯度下降法/多项式.png" alt></p>
<p>比如二次方模型$h<em>\theta (x) = \theta_0+\theta_1x+\theta_2x^2$或者三次方模型$h</em>\theta (x) = \theta_0+\theta_1x+\theta_2x^2+\theta_3 x^3$。我们可以令$x_2 = x^2,x_3=x^3$将多项式模型转化为线性回归模型。</p>
<blockquote>
<p>如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</p>
</blockquote>
<h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><p>在我们面对多维特征问题的时候，我们要<strong>保证这些特征都具有相近的尺度</strong>，这将帮助梯度下降算法更快地收敛。如果特征范围相差过大，则梯度下降法需要非常多次的迭代才能收敛。</p>
<p>解决的方法是尝试将所有特征的尺度都尽量缩放到$-1$到$1$之间，最简单的方法是令</p>
<script type="math/tex; mode=display">
x_n=\frac{x_n-\mu_n}{s_n}</script><p>其中$\mu_n$是平均值，$s_n$是标准差。</p>
<p>在某些将问题中，可以将特征进行人为的组合以简化模型：比如某个房价预测问题$h<em>\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2$，$x_1$为房屋的临街宽度，$x_2$为房屋的纵向深度，如果我们令$x= x_1\times x_2$来表示房屋的面积，则$h</em>\theta(x)=\theta_0+\theta_1x$。</p>
<h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><p>梯度下降算法的每次迭代受到学习率的影响，如果学习率$\alpha$过小，则达到收敛所需的迭代次数会非常高；如果学习率$\alpha$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试些学习率：$\alpha = 0.01,~0.03,~0.1,~0.3,~1,~3,~10$</p>
<h2 id="随机梯度下降-Stochastic-Gradient-Descent"><a href="#随机梯度下降-Stochastic-Gradient-Descent" class="headerlink" title="随机梯度下降(Stochastic Gradient Descent)"></a>随机梯度下降(Stochastic Gradient Descent)</h2><p>如果我们有一个很大的训练集，那么每次迭代都要对他们求和，如果硬件设备不那么好就会很浪费时间。可以用<strong>随机梯度下降法</strong>(SGD)来代替批量梯度下降法。</p>
<p>在随机梯度下降法中，定义代价函数为一个单一训练实例的代价：</p>
<script type="math/tex; mode=display">
cost \left( 
\theta,(x^{(i)},y^{(i)})\right)
=\frac{1}{2}
\left(h_\theta (x^{(i)})-y^{(i)}\right)^2</script><p>首先随机打乱样本顺序，之后对$\theta$进行迭代：</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha \left(h_\theta (x^{(i)})-y^{(i)}\right)x^{(i)}_j</script><p>随机梯度下降算法在只用一个样本计算之后便更新参数$\theta$，直至所有样本都参与迭代1~10次。但SGD不是每一步都是朝着“正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。</p>
<p>我们可以令学习率$\alpha$随着迭代次数的增加而减小，比如：</p>
<script type="math/tex; mode=display">
\alpha = \frac {A} {\text{Iteration Num}+B}</script><p>通过减小学习率，随着迭代次数的增加，我们不断地靠近全局最小值迫使算法收敛而非在最小值附近徘徊。 但是通常我们不需要这样做便能有非常好的效果了，对$\alpha$进行调整所耗费的计算不太值得。</p>
<h2 id="小批量梯度下降-Mini-Batch-Gradient-Descent"><a href="#小批量梯度下降-Mini-Batch-Gradient-Descent" class="headerlink" title="小批量梯度下降(Mini-Batch Gradient Descent)"></a>小批量梯度下降(Mini-Batch Gradient Descent)</h2><p>小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$个训练实例，便更新一次参数$\theta$。 </p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\frac{\alpha}{b} \sum_{k=i}^{i-1+b} \left(h_\theta (x^{(k)})-y^{(k)}\right)x^{(k)}_j</script>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          
            <a href="/tags/监督学习/" rel="tag"><i class="fa fa-tag"></i> 监督学习</a>
          
            <a href="/tags/回归/" rel="tag"><i class="fa fa-tag"></i> 回归</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/09/16/机器学习资料/" rel="next" title="资料整理">
                <i class="fa fa-chevron-left"></i> 资料整理
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/09/正规方程法/" rel="prev" title="线性回归">
                线性回归 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar1.jpg" alt="Fangchen Wu">
            
              <p class="site-author-name" itemprop="name">Fangchen Wu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">38</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/FrozenWu" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:lalalafrozen@live.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/2881393720" target="_blank" title="Weibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Weibo</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/wufangchen95" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#方向导数与梯度"><span class="nav-number">1.</span> <span class="nav-text">方向导数与梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#导数"><span class="nav-number">1.1.</span> <span class="nav-text">导数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#方向导数"><span class="nav-number">1.2.</span> <span class="nav-text">方向导数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度"><span class="nav-number">1.3.</span> <span class="nav-text">梯度</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度下降法-Gradient-Descent"><span class="nav-number">2.</span> <span class="nav-text">梯度下降法(Gradient Descent)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#单变量线性回归"><span class="nav-number">2.1.</span> <span class="nav-text">单变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数-Cost-Function"><span class="nav-number">2.1.1.</span> <span class="nav-text">代价函数(Cost Function)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降"><span class="nav-number">2.1.2.</span> <span class="nav-text">梯度下降</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多变量线性回归"><span class="nav-number">2.2.</span> <span class="nav-text">多变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#多变量梯度下降"><span class="nav-number">2.2.1.</span> <span class="nav-text">多变量梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多项式回归"><span class="nav-number">2.2.2.</span> <span class="nav-text">多项式回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征缩放"><span class="nav-number">2.2.3.</span> <span class="nav-text">特征缩放</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习率"><span class="nav-number">2.2.4.</span> <span class="nav-text">学习率</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机梯度下降-Stochastic-Gradient-Descent"><span class="nav-number">2.3.</span> <span class="nav-text">随机梯度下降(Stochastic Gradient Descent)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小批量梯度下降-Mini-Batch-Gradient-Descent"><span class="nav-number">2.4.</span> <span class="nav-text">小批量梯度下降(Mini-Batch Gradient Descent)</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fangchen Wu</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">全站字数统计&#58;</span>
    
    <span title="全站字数统计">59.3k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
